{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/231FA04478/AI-AGENT/blob/main/IMDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1CxozMu8d78",
        "outputId": "91b7416e-767e-4dd9-9707-1b72ba2d9f85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from bs4 import BeautifulSoup          # BeautifulSoup is a useful library for extracting data from HTML and XML documents\n",
        "from numpy import array\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Activation, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, GlobalMaxPooling1D, Dense, Embedding, LSTM, GRU\n",
        "import pandas.testing as tm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sXOv1aBGCuyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the Dataset\n",
        "movie_reviews = pd.read_csv(\"/content/IMDB Dataset.csv\")"
      ],
      "metadata": {
        "id": "KNZqYVLuAEki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Check the shape of the data\n",
        "movie_reviews.shape"
      ],
      "metadata": {
        "id": "AG7EQfEKAEh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "movie_reviews.head()"
      ],
      "metadata": {
        "id": "-5sRVlPNAEez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Check for null Values\n",
        "movie_reviews.isnull().sum()\n"
      ],
      "metadata": {
        "id": "AvKVcB-fAV_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us view one of the reviews\n",
        "movie_reviews[\"review\"][5]"
      ],
      "metadata": {
        "id": "TMDzXZ4iAZFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the distribution of positive and negative sentiments in the dataset\n",
        "sns.countplot(x='sentiment', data = movie_reviews)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FPdNnNOMAV8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing the html strips\n",
        "def strip_html(text):\n",
        "    # BeautifulSoup is a useful library for extracting data from HTML and XML documents\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()"
      ],
      "metadata": {
        "id": "eGXRRw46AV5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "movie_reviews['review'] = movie_reviews['review'].apply(strip_html)"
      ],
      "metadata": {
        "id": "8COszl9lAV0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing punctuations\n",
        "def remove_punctuations(text):\n",
        "\n",
        "    pattern = r'[^a-zA-Z0-9\\s]'\n",
        "    text = re.sub(pattern,'',text)\n",
        "\n",
        "    # Single character removal\n",
        "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', text)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "rdhSlCKzAjDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Remove puntuations\n",
        "movie_reviews['review'] = movie_reviews['review'].apply(remove_punctuations)"
      ],
      "metadata": {
        "id": "JdHDZy_TAi-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting english stopwords\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "print(stopword_list)\n"
      ],
      "metadata": {
        "id": "BGNWHiDZArWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Exclude 'not' and its other forms from the stopwords list\n",
        "\n",
        "updated_stopword_list = []\n",
        "\n",
        "for word in stopword_list:\n",
        "    if word=='not' or word.endswith(\"n't\"):\n",
        "        pass\n",
        "    else:\n",
        "        updated_stopword_list.append(word)\n",
        "\n",
        "print(updated_stopword_list)"
      ],
      "metadata": {
        "id": "5WfR3xEsArTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk\n"
      ],
      "metadata": {
        "id": "yUNT3sBdG6eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n"
      ],
      "metadata": {
        "id": "jsEFv9_ZA5Y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download()\n"
      ],
      "metadata": {
        "id": "iWHync4zHFuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "# Download the 'punkt_tab' resource\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "print(word_tokenize(\"This is a test.\"))  # Should work now"
      ],
      "metadata": {
        "id": "52Pfsz-WA5Po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "KTpb83JwBSdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download required resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    words = word_tokenize(text)\n",
        "    filtered = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered)\n",
        "\n",
        "# Apply to your DataFrame\n",
        "movie_reviews['review'] = movie_reviews['review'].apply(remove_stopwords)\n"
      ],
      "metadata": {
        "id": "DdSi5fbsBSaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Import the NLTK library\n",
        "# import nltk\n",
        "\n",
        "# # Download the 'punkt_tab' resource\n",
        "# nltk.download('punkt_tab')\n",
        "\n",
        "# # setting english stopwords\n",
        "# stopword_list = nltk.corpus.stopwords.words('english')\n",
        "# print(stopword_list)\n",
        "\n",
        "# # Exclude 'not' and its other forms from the stopwords list\n",
        "# updated_stopword_list = []\n",
        "\n",
        "# for word in stopword_list:\n",
        "#     if word=='not' or word.endswith(\"n't\"):\n",
        "#         pass\n",
        "#     else:\n",
        "#         updated_stopword_list.append(word)\n",
        "\n",
        "# print(updated_stopword_list)\n",
        "\n",
        "# # removing the stopwords\n",
        "# def remove_stopwords(text, is_lower_case=False):\n",
        "#     # splitting strings into tokens (list of words)\n",
        "#     tokens = nltk.tokenize.word_tokenize(text)\n",
        "#     tokens = [token.strip() for token in tokens]\n",
        "#     if is_lower_case:\n",
        "#         # filtering out the stop words\n",
        "#         filtered_tokens = [token for token in tokens if token not in updated_stopword_list]\n",
        "#     else:\n",
        "#         filtered_tokens = [token for token in tokens if token.lower() not in updated_stopword_list]\n",
        "#     filtered_text = ' '.join(filtered_tokens)\n",
        "#     return filtered_text\n",
        "\n",
        "# # Apply function on review column\n",
        "# movie_reviews['review'] = movie_reviews['review'].apply(remove_stopwords)\n"
      ],
      "metadata": {
        "id": "IbUsCv0oBSXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_reviews['review'] = movie_reviews['review'].apply(remove_stopwords)"
      ],
      "metadata": {
        "id": "q4BQiDRlBYaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_reviews.head()"
      ],
      "metadata": {
        "id": "uYOmV0-nBYO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert sentiment labels to integers\n",
        "\n",
        "movie_reviews['sentiment'] = movie_reviews['sentiment'].apply(lambda x: 1 if x==\"positive\" else 0)"
      ],
      "metadata": {
        "id": "gd3j0eOyBYM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "movie_reviews.head()"
      ],
      "metadata": {
        "id": "zxx0L7lNBkK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(movie_reviews['review'].values, movie_reviews['sentiment'].values,\n",
        "                                                    test_size=0.20,\n",
        "                                                    random_state=42)\n",
        "len(X_train), len(X_test), len(y_train), len(y_test)"
      ],
      "metadata": {
        "id": "nq78bMDgBnTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Tokenizer class from the keras.preprocessing.text module creates a word-to-index dictionary\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_tok = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_tok = tokenizer.texts_to_sequences(X_test)\n"
      ],
      "metadata": {
        "id": "SJQ6RJZ9BnQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Find the vocabulary size and perform padding on both train and test set\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "maxlen = 100\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_tok, padding='post', maxlen=maxlen, truncating='post')\n",
        "X_test_pad = pad_sequences(X_test_tok, padding='post', maxlen=maxlen, truncating='post')"
      ],
      "metadata": {
        "id": "pyDWBgxoC7K0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print ('number of unique words in the corpus:', vocab_size)"
      ],
      "metadata": {
        "id": "iS41VI0tC7D-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "EMBEDDING_DIM = 32\n",
        "\n",
        "print('Build model...')\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=maxlen))\n",
        "model.add(LSTM(64, return_sequences=True))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Try using different optimizers and different optimizer configs\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print('Summary of the built model...')\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "rPTWszPFC66s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "history = model.fit(X_train_pad, y_train, batch_size=128, epochs=1, verbose=1, validation_split=0.2)"
      ],
      "metadata": {
        "id": "AhsxFjq_DqVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print('Testing...')\n",
        "y_test = np.array(y_test)\n",
        "score, acc = model.evaluate(X_test_pad, y_test, batch_size=128)\n",
        "\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)\n",
        "\n",
        "print(\"Accuracy: {0:.2%}\".format(acc))"
      ],
      "metadata": {
        "id": "OuFBMFgGD0sK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let us test some  samples\n",
        "test_sample_1 = \"This movie is fantastic! I really like it because it is so good!\"\n",
        "test_sample_2 = \"Good movie!\"\n",
        "test_sample_3 = \"Maybe I like this movie.\"\n",
        "test_sample_4 = \"Not to my taste, will skip and watch another movie\"\n",
        "test_sample_5 = \"if you like action, then this movie might be good for you.\"\n",
        "test_sample_6 = \"Bad movie!\"\n",
        "test_sample_7 = \"Not a good movie!\"\n",
        "test_sample_8 = \"This movie really sucks! Can I get my money back please?\"\n",
        "test_samples = [test_sample_1, test_sample_2, test_sample_3, test_sample_4, test_sample_5, test_sample_6, test_sample_7, test_sample_8]\n",
        "\n",
        "test_samples_tokens = tokenizer.texts_to_sequences(test_samples)\n",
        "test_samples_tokens_pad = pad_sequences(test_samples_tokens, maxlen=maxlen)\n",
        "\n",
        "# predict\n",
        "pred = model.predict(x=test_samples_tokens_pad)\n",
        "pred\n"
      ],
      "metadata": {
        "id": "yDN4V6BqEAvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap\n",
        "!pip install lime\n",
        "!pip install eli5\n",
        "!pip install alibi"
      ],
      "metadata": {
        "id": "jwyfBrUEEAtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lime\n",
        "import lime.lime_text\n",
        "import numpy as np\n",
        "\n",
        "# Assuming model and X_test are ready from before\n",
        "\n",
        "# Create a LimeTextExplainer\n",
        "# The class_names should match your model's output classes (0 for negative, 1 for positive)\n",
        "explainer = lime.lime_text.LimeTextExplainer(class_names=['Negative', 'Positive'])\n",
        "\n",
        "# Define a prediction function that Lime can use\n",
        "# Lime expects a function that takes a list of strings and returns probabilities for each class\n",
        "def predict_proba_for_lime(text_list):\n",
        "    # Tokenize and pad the text list\n",
        "    text_tokens = tokenizer.texts_to_sequences(text_list)\n",
        "    text_pad = pad_sequences(text_tokens, maxlen=maxlen)\n",
        "    # Get predictions from the model\n",
        "    predictions = model.predict(text_pad)\n",
        "    # Lime expects probabilities for each class. Since your model outputs a single value (sigmoid),\n",
        "    # we need to create a 2D array where the second column is 1 - prediction.\n",
        "    # This represents the probability for the negative class.\n",
        "    return np.hstack((1 - predictions, predictions))\n",
        "\n",
        "# Explain the first test instance (using the original text before tokenization/padding)\n",
        "i = 0\n",
        "# Get the original review text from X_test (which is still a NumPy array of strings)\n",
        "original_text_to_explain = X_test[i]\n",
        "\n",
        "exp = explainer.explain_instance(\n",
        "    text_instance=original_text_to_explain,\n",
        "    classifier_fn=predict_proba_for_lime,\n",
        "    num_features=5 # Number of features (words) to highlight in the explanation\n",
        ")\n",
        "\n",
        "# Show explanation\n",
        "# Removed the 'show_table=True' argument\n",
        "exp.show_in_notebook()\n"
      ],
      "metadata": {
        "id": "C_1IRrmFEAqC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}